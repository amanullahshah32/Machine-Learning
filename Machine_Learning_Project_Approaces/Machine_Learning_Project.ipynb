{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNLZNiziUGxFnOItHgyeKhp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanullahshah32/Machine-Learning/blob/main/Machine_Learning_Project_Approaces/Machine_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "jupyter runtime code:\n",
        "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0\n"
      ],
      "metadata": {
        "id": "5h2p3X1AWRXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps for   approaching ML problems:\n",
        "\n",
        "1. Understand the business requirements and the nature of the available data.\n",
        "2. Classify the problem as supervised/unsupervised and regression/classification.\n",
        "3. Download, clean & explore the data and create new features that may improve models.\n",
        "4. Create training/test/validation sets and prepare the data for training ML models.\n",
        "5. Create a quick & easy baseline model to evaluate and benchmark future models.\n",
        "6. Pick a modeling strategy, train a model, and tune hyperparameters to achieve optimal fit.\n",
        "7. Experiment and combine results from multiple strategies to get a better result.\n",
        "8. Interpret models, study individual predictions, and present your findings."
      ],
      "metadata": {
        "id": "dE-a1o0f-mPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh9TcwOa-OuM"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "matplotlib.rcParams['font.size'] = 14\n",
        "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
        "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 - Understand Business Requirements & Nature of Data\n",
        "\n",
        "<img src=\"https://i.imgur.com/63XEArk.png\" width=\"640\">\n",
        "\n",
        "\n",
        "Most machine learning models are trained to serve a real-world use case. It's important to understand the business requirements, modeling objectives and the nature of the data available before you start building a machine learning model."
      ],
      "metadata": {
        "id": "VhV-DHFbFG5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Big Picture\n",
        "\n",
        "The first step in any machine learning problem is to read the given documentation, talk to various stakeholders and identify the following:\n",
        "\n",
        "1. What is the business problem you're trying to solve using machine learning?\n",
        "2. Why are we interested in solving this problem? What impact will it have on the business?\n",
        "3. How is this problem solved currently, without any machine learning tools?\n",
        "4. Who will use the results of this model, and how does it fit into other business processes?\n",
        "5. How much historical data do we have, and how was it collected?\n",
        "6. What features does the historical data contain? Does it contain the historical values for what we're trying to predict.\n",
        "7. What are some known issues with the data (data entry errors, missing data, differences in units etc.)\n",
        "8. Can we look at some sample rows from the dataset? How representative are they of the entire dataset.\n",
        "9. Where is the data stored and how will you get access to it?\n",
        "10. ...\n",
        "\n",
        "\n",
        "Gather as much information about the problem as possible, so that you're clear understanding of the objective and feasibility of the project."
      ],
      "metadata": {
        "id": "Uau5bu7EG8j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Classify the problem as supervised/unsupervised & regression/classification\n",
        "\n",
        "<img src=\"https://i.imgur.com/rqt2A7F.png\" width=\"640\">\n",
        "\n",
        "Here's the landscape of machine learning([source](https://medium.datadriveninvestor.com/machine-learning-in-10-minutes-354d83e5922e)):\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/842/1*tlQwBmbL6RkuuFq8OPJofw.png\" width=\"640\">"
      ],
      "metadata": {
        "id": "JoMX5im-b7HW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Here are the topics in machine learning that we're studying in this course ([source](https://vas3k.com/blog/machine_learning/)):\n",
        "\n",
        "<img src=\"https://i.imgur.com/VbVFAsg.png\" width=\"640\">\n",
        "\n"
      ],
      "metadata": {
        "id": "qAjuWCI_cGSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Functions and Evaluation Metrics\n",
        "\n",
        "Once you have identified the type of problem you're solving, you need to pick an appropriate evaluation metric. Also, depending on the kind of model you train, your model will also use a loss/cost function to optimize during the training process.\n",
        "\n",
        "* **Evaluation metrics** - they're used by humans to evaluate the ML model\n",
        "\n",
        "* **Loss functions** - they're used by computers to optimize the ML model\n",
        "\n",
        "They are often the same (e.g. RMSE for regression problems), but they can be different (e.g. Cross entropy and Accuracy for classification problems).\n",
        "\n",
        "See this article for a survey of common loss functions and evaluation metrics: https://towardsdatascience.com/11-evaluation-metrics-data-scientists-should-be-familiar-with-lessons-from-a-high-rank-kagglers-8596f75e58a7"
      ],
      "metadata": {
        "id": "KjudBoLxdCwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7hbmhsTH_b-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ross_df = pd.read_csv('/content/drive/MyDrive/Datasets/rossmann-store-sales/train.csv')"
      ],
      "metadata": {
        "id": "1GcbhHrCfoBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ross_df"
      ],
      "metadata": {
        "id": "2sFuVHfDgFvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_df = pd.read_csv('/content/drive/MyDrive/Datasets/rossmann-store-sales/store.csv')"
      ],
      "metadata": {
        "id": "DMDaliqagIHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_df\n"
      ],
      "metadata": {
        "id": "2BpWBVOFgOYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### we can merge the two data freames to get a richer set of feature for each row of the training set"
      ],
      "metadata": {
        "id": "BFTnsOzWghTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = ross_df.merge(store_df,  on='Store')"
      ],
      "metadata": {
        "id": "Y8caeFhLgQNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "fRner_m9gzyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.shape"
      ],
      "metadata": {
        "id": "fdmzhu0vg1lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('/content/drive/MyDrive/Datasets/rossmann-store-sales/test.csv')"
      ],
      "metadata": {
        "id": "-E5ymv3LhLqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df = test_df.merge(store_df, on='Store')"
      ],
      "metadata": {
        "id": "W-UiC_4Cdx7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df"
      ],
      "metadata": {
        "id": "TSWuZBR4eDOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning Data\n",
        "\n",
        "### the first step is to check the column data types and identify if there are any null values"
      ],
      "metadata": {
        "id": "Vi6169ONJzeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.info()"
      ],
      "metadata": {
        "id": "xSnK4DwVeEwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round(merged_df.describe().T,2)"
      ],
      "metadata": {
        "id": "MHSFHLH8KDl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Store'].unique()"
      ],
      "metadata": {
        "id": "YXnLx8Kue8I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df= merged_df[merged_df['Store']>3]"
      ],
      "metadata": {
        "id": "azoVmN_prbfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df"
      ],
      "metadata": {
        "id": "rQzAKmPVso2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "bWwx3cz-tUfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Date'] = pd.to_datetime(merged_df.Date)"
      ],
      "metadata": {
        "id": "JoobR4Tgt9Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df['Date']= pd.to_datetime(merged_test_df.Date)"
      ],
      "metadata": {
        "id": "s6yRgtzUuL-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "0skbvOHWuTw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.Date.min(), merged_df['Date'].max()"
      ],
      "metadata": {
        "id": "hDK6CMjkvj8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df.Date.min(), merged_test_df.Date.max()"
      ],
      "metadata": {
        "id": "W2REmEEAvl_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis and Visualization\n",
        "\n",
        "Objectives of exploratory data analysis:\n",
        "\n",
        "- Study the distributions of individual columns (uniform, normal, exponential)\n",
        "- Detect anomalies or errors in the data (e.g. missing/incorrect values)\n",
        "- Study the relationship of target column with other columns (linear, non-linear etc.)\n",
        "- Gather insights about the problem and the dataset\n",
        "- Come up with ideas for preprocessing and feature engineering\n",
        "\n"
      ],
      "metadata": {
        "id": "04v-bI5a0jfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(rc={'figure.figsize':(11.7,8.27)})"
      ],
      "metadata": {
        "id": "Y2TZS6XR3Ub9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.histplot(data = merged_df, x='Sales')"
      ],
      "metadata": {
        "id": "vnsD7J5wvrBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store closes analysis,\n",
        "# sales are zero on the day when the store is close\n",
        "\n",
        "merged_df.Open.value_counts()"
      ],
      "metadata": {
        "id": "IHilIuFN1pbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(merged_df['Sales']==0).sum()"
      ],
      "metadata": {
        "id": "Jt4pFLJA8N4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.Sales.value_counts()[0]"
      ],
      "metadata": {
        "id": "RSoBe_mr9Dm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### delete rows where the store is closed"
      ],
      "metadata": {
        "id": "gLi99Ihj-kuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exclude the dates when store was closed\n",
        "merged_df = merged_df[merged_df.Open==1].copy()"
      ],
      "metadata": {
        "id": "syRGUj18AcqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df\n"
      ],
      "metadata": {
        "id": "5SIUUWvyBWr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(merged_df, x='Sales')"
      ],
      "metadata": {
        "id": "RUlQnLTHBYMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.describe()"
      ],
      "metadata": {
        "id": "jq6pFGPLnkm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = merged_df.sample(40000)"
      ],
      "metadata": {
        "id": "CV_c7XK6oucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.scatterplot(x= temp_df.Sales, y = temp_df.Customers, hue= temp_df.Date.dt.year, alpha = 0.8)\n",
        "plt.title('Sales vs Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PGNNNL3poCar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the total customers per year\n",
        "\n",
        "total_customers_per_year = temp_df.groupby(temp_df['Date'].dt.year)['Customers'].sum()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=total_customers_per_year.index, y=total_customers_per_year.values, color='skyblue')\n",
        "plt.title('Total Customers per Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Total Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qpsj661Dpct7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = merged_df.sample(10000)\n",
        "sns.scatterplot(x = temp_df.Store, y= temp_df.Sales, hue=  temp_df.Date.dt.year, alpha =0.8)\n",
        "plt.title(\"stores vs sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "23ABBsvYqFaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=merged_df, x='DayOfWeek', y='Sales', hue=True)"
      ],
      "metadata": {
        "id": "_4-0wBkV0UV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(merged_df, x='Promo', y ='Sales', palette='husl')"
      ],
      "metadata": {
        "id": "BR1xBdEf0cZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "P7BjatRgzP8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.Promo.unique()"
      ],
      "metadata": {
        "id": "d81kJAG40VR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.corr()['Sales'].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "qGL58S6R0bTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.Sales.corr(merged_df.Promo)"
      ],
      "metadata": {
        "id": "iBJ-uYDj0tuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.corr()"
      ],
      "metadata": {
        "id": "4plfw1sb1l5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(merged_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('COrrelationMatrx')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LeuAqvzm1uXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**\n",
        "\n",
        "Feature engineer is the process of creating new features (columns) by transforming/combining existing features or by incorporating data from external sources.\n",
        "\n",
        "\n",
        "For example, here are some features that can be extracted from the \"Date\" column:\n",
        "\n",
        "1. Day of week\n",
        "2. Day or month\n",
        "3. Month\n",
        "4. Year\n",
        "5. Weekend/Weekday\n",
        "6. Month/Quarter End\n"
      ],
      "metadata": {
        "id": "OrhyJ_CLxC0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Day'] = merged_df.Date.dt.day\n",
        "merged_df['Month'] = merged_df.Date.dt.month\n",
        "merged_df['Year'] = merged_df.Date.dt.year"
      ],
      "metadata": {
        "id": "JJ7mBm2r24Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df['Day'] = merged_test_df.Date.dt.day\n",
        "merged_test_df['Month'] = merged_test_df.Date.dt.month\n",
        "merged_test_df['Year'] = merged_test_df.Date.dt.year"
      ],
      "metadata": {
        "id": "3hMHBnOQ2JUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(merged_df, x='Year',y='Sales', palette='husl')"
      ],
      "metadata": {
        "id": "H3QSs7yK2cCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df= merged_df[merged_df['Year']==2015]\n",
        "#sns.barplot(data= filtered_df, x='Month', y='Sales', palette='husl')"
      ],
      "metadata": {
        "id": "ZER-tX702iCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data= merged_df, x='Month', y='Sales', palette='husl')"
      ],
      "metadata": {
        "id": "aRGgLPGGk1b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_columns = None # to show all the column names in the dataFrame"
      ],
      "metadata": {
        "id": "X74ZXlHimDX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "sKbOSdlTjJgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merged_df.Year.value_counts()"
      ],
      "metadata": {
        "id": "cMaFALSOjOBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Using date information, we can also create new current columns like:\n",
        "\n",
        "1. Weather on each day\n",
        "2. Whether the date was a public holiday\n",
        "3. Whether the store was running a promotion on that day.\n",
        "\n",
        "\n",
        "> **EXERCISE**: Create new columns using the above ideas."
      ],
      "metadata": {
        "id": "Ywkose2AlVcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.columns"
      ],
      "metadata": {
        "id": "W6WpGXlxjTQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### comparison between holidays vs sales.\n",
        "\n",
        "1. compare sales of state holiday\n",
        "2. compare sales of school holiday"
      ],
      "metadata": {
        "id": "3Xlvpp74pPiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "compare sales of state holiday"
      ],
      "metadata": {
        "id": "PVfOXfJ2vKlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "\n",
        "\n",
        "state_holiday= merged_df[merged_df['StateHoliday']==1]\n",
        "non_state_holiday= merged_df[merged_df['StateHoliday']==0]\n",
        "\n",
        "# Plot sales during state holidays\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(data=state_holiday, x='Month', y='Sales', palette='husl')\n",
        "plt.title('Sales during State Holidays')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "\n",
        "# Plot sales during non-state holidays\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(data=non_state_holiday, x='Month', y='Sales', palette='husl')\n",
        "plt.title('Sales during Non-State Holidays')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yT4I0taslvD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total sales during state holidays and non-holidays\n",
        "total_state_holiday_sales = merged_df[merged_df['StateHoliday'] == 1]['Sales'].sum()\n",
        "total_non_holiday_sales = merged_df[merged_df['StateHoliday'] == 0]['Sales'].sum()\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "sales_comparison_df = pd.DataFrame({\n",
        "    'State Holiday': total_state_holiday_sales,\n",
        "    'Non-Holiday': total_non_holiday_sales\n",
        "}, index=['Total Sales'])\n",
        "\n",
        "# Plot the bar plot to compare total sales during state holidays and non-holidays\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=sales_comparison_df, palette='husl')\n",
        "plt.title('Total Sales Comparison: State Holiday vs. Non-Holiday')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KNFJCxx7qXPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compare sales of school holiday"
      ],
      "metadata": {
        "id": "sK-kasnmvOKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "school_holiday = merged_df[merged_df['SchoolHoliday']==1]\n",
        "non_school_holiday = merged_df[merged_df['SchoolHoliday']==0]\n",
        "\n",
        "#plot sales during school holidays\n",
        "plt.subplot(1,2,1)\n",
        "sns.barplot(school_holiday, x=\"Month\", y='Sales', palette='husl')\n",
        "plt.title('sales during school holiday')\n",
        "plt.xlabel('month')\n",
        "plt.ylabel('sales')\n",
        "\n",
        "#plot sales during non school holidays\n",
        "plt.subplot(1,2,2)\n",
        "sns.barplot(non_school_holiday, x=\"Month\", y='Sales', palette='husl')\n",
        "plt.title('sales during non school holiday')\n",
        "plt.xlabel('month')\n",
        "plt.ylabel('sales')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y-UDVx2ltzFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "H-3qKt0jw3qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HW"
      ],
      "metadata": {
        "id": "NwtUjG1yZvNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **EXERCISE**: The features `Promo2`, `Promo2SinceWeek` etc. are not very useful in their current form, because they do not relate to the current date. How can you improve their representation?"
      ],
      "metadata": {
        "id": "v2_Sy1dLZuFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 - Create a training/test/validation split and prepare the data for training\n",
        "\n",
        "<img src=\"https://i.imgur.com/XZ9aP10.png\" width=\"640\">"
      ],
      "metadata": {
        "id": "wlJTt3D4ZxNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/Test/Validation Split\n",
        "\n",
        "The data already contains a test set, which contains over one month of data after the end of the training set. We can apply a similar strategy to create a validation set. We'll the last 25% of rows for the validation set, after ordering by date"
      ],
      "metadata": {
        "id": "MFtsppKGumHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "JixrzV-1ZmUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the unique days in test dataset\n",
        "\n",
        "merged_test_df['Date'] = pd.to_datetime(merged_test_df['Date'])\n",
        "date = merged_test_df['Date'].dt.date\n",
        "num_days = len(date.unique())\n",
        "num_days"
      ],
      "metadata": {
        "id": "BbFm8f3KudUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(merged_df)"
      ],
      "metadata": {
        "id": "_145yosEvElc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(.75 * len(merged_df))\n",
        "train_size"
      ],
      "metadata": {
        "id": "kwM8M5qAvqoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_df  = merged_df.sort_values('Date')\n",
        "train_df, val_df = sorted_df[:train_size], sorted_df[train_size:]"
      ],
      "metadata": {
        "id": "mgZ16Gq0v2UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df) , len(val_df)"
      ],
      "metadata": {
        "id": "kAHle4RAv4Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "m1Gbx8CIIoUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df"
      ],
      "metadata": {
        "id": "SQc2chlSJXMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.Date.min(), train_df.Date.max()"
      ],
      "metadata": {
        "id": "FXdwT1FiJYGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.Date.min(), val_df.Date.max()"
      ],
      "metadata": {
        "id": "qJ-pG_u6Jm7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df.Date.min(), merged_test_df.Date.max()"
      ],
      "metadata": {
        "id": "a9RRImyGJoGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns.values.tolist(), len(train_df.columns)"
      ],
      "metadata": {
        "id": "jcMOJ6gLJ2JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df.columns.values.tolist(), len(merged_test_df.columns)"
      ],
      "metadata": {
        "id": "8HB09D4bKRMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input and Target columns\n",
        "\n",
        "Let's also identify input and target columns. Note that we can't use the no. of customers as an input, because this information isn't available beforehand. Also, we needn't use all the available columns, we can start out with just a small subset."
      ],
      "metadata": {
        "id": "cXv7CDNeLysW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_cols = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'StoreType', 'Assortment', 'Day', 'Month', 'Year']"
      ],
      "metadata": {
        "id": "PZfirOUkK-pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_col = 'Sales'"
      ],
      "metadata": {
        "id": "y_PrOQtRL96P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df[input_cols].nunique()"
      ],
      "metadata": {
        "id": "wTsg35T0MEyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = train_df[input_cols].copy()\n",
        "train_targets = train_df[target_col].copy()"
      ],
      "metadata": {
        "id": "ULUwd8BuMLiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_inputs = val_df[input_cols].copy()\n",
        "val_targets = val_df[target_col].copy()"
      ],
      "metadata": {
        "id": "T-09E7mUMyG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = merged_test_df[input_cols].copy()\n",
        "# Test data does not have targets"
      ],
      "metadata": {
        "id": "Mb86eE7wM9YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that some columns can be treated as both numeric and categorical, and it's up to you to decide how you want to deal with them."
      ],
      "metadata": {
        "id": "VhUZqCFXOzTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = ['Store', 'Day', 'Month', 'Year']\n",
        "categorical_cols = ['DayOfWeek', 'Promo', 'StateHoliday', 'StoreType', 'Assortment']"
      ],
      "metadata": {
        "id": "bAWOOXqTO16f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data types check\n",
        "\n",
        "numeric_cols = ['Store', 'Day', 'Month', 'Year']\n",
        "categorical_cols = ['DayOfWeek', 'Promo', 'StateHoliday', 'StoreType', 'Assortment']\n",
        "\n",
        "numeric_data_types = {col: 'int' for col in numeric_cols}\n",
        "categorical_data_types = {col: 'object' for col in categorical_cols}\n",
        "\n",
        "print(\"Numeric Columns Data Types:\")\n",
        "print(numeric_data_types)\n",
        "\n",
        "print(\"\\nCategorical Columns Data Types:\")\n",
        "print(categorical_data_types)\n"
      ],
      "metadata": {
        "id": "wrUwXdf2wpsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputation, Scaling and Encode\n",
        "\n",
        "Let's impute missing data from numeric columns and scale the values to the $(0, 1)$ range."
      ],
      "metadata": {
        "id": "jAeTZpJPv-7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "WVpLGXSpt2tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy='mean').fit(train_inputs[numeric_cols])"
      ],
      "metadata": {
        "id": "V6kXluyPxS_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])\n",
        "val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols])\n",
        "test_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols])"
      ],
      "metadata": {
        "id": "Vjd52CaKyR-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's encode categorical columns as one-hot vectors."
      ],
      "metadata": {
        "id": "mEEUZeHpnh59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "OvTso4gNyVop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(train_inputs[categorical_cols])\n",
        "encoded_cols = list(encoder.get_feature_names_out(categorical_cols))"
      ],
      "metadata": {
        "id": "fBy3MveExUtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
        "val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
        "test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])"
      ],
      "metadata": {
        "id": "MXHW-oazuHcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the `scikit-learn` preprocessing module: https://scikit-learn.org/stable/modules/preprocessing.html"
      ],
      "metadata": {
        "id": "vBt-vayTmJuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now extract out the numeric data."
      ],
      "metadata": {
        "id": "7nRxRW45mLfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_inputs[numeric_cols + encoded_cols]\n",
        "X_val = val_inputs[numeric_cols + encoded_cols]\n",
        "X_test = test_inputs[numeric_cols + encoded_cols]"
      ],
      "metadata": {
        "id": "cYipivB3mKFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 - Create quick & easy baseline models to benchmark future models\n",
        "\n",
        "<img src=\"https://i.imgur.com/1DLgiEz.png\" width=\"640\">"
      ],
      "metadata": {
        "id": "ya3-A4PCvsrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick baseline model helps establish the minimum score any ML model you train should achieve.\n",
        "\n",
        "\n",
        "### Fixed/Random Guess\n",
        "\n",
        "Let's define a model that always returns the mean value of Sales as the prediction."
      ],
      "metadata": {
        "id": "TJ4cw3K2vx62"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXZsoSWDvthV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}